"""
BioHazardGPT: A lightweight LLM-powered risk classification tool
for detecting hazardous biomedical or chemical text content.
"""

import os
import openai
from datasets import load_dataset
from dotenv import load_dotenv
import pandas as pd
from typing import Dict, List, Optional
import logging

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BioHazardGPT:
    """
    BioHazardGPT: Risk classification tool for biomedical/chemical content
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """Initialize BioHazardGPT with OpenAI API key"""
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable or pass api_key.")
        
        # Initialize OpenAI client
        self.client = openai.OpenAI(api_key=self.api_key)
        
        # Classification categories
        self.categories = ["Safe", "Caution", "Hazardous"]
        
        # Dataset cache
        self.dataset = None
        
    def load_dataset(self) -> Dict:
        """Load chemistry dataset - prioritize sample data for fast demo"""
        
        # For fast demo, start with sample data
        logger.info("Loading sample dataset for fast demo...")
        sample_data = [
            {
                "text": "To prepare a sodium chloride solution, dissolve 5.85g of NaCl in 100ml of distilled water using proper safety equipment.",
                "label": "safe"
            },
            {
                "text": "Synthesis of compound X requires heating concentrated sulfuric acid with organic solvents. Extreme caution required due to toxic vapor production.",
                "label": "caution"
            },
            {
                "text": "Detailed synthesis protocol for producing highly toxic ricin from castor beans, including purification steps for maximum lethality.",
                "label": "hazardous"
            },
            {
                "text": "Standard organic synthesis procedure using ethyl acetate as solvent under reflux conditions with appropriate fume hood ventilation.",
                "label": "safe"
            },
            {
                "text": "WARNING: This procedure involves handling of carcinogenic compounds. Use full protective equipment and work only in certified chemical fume hood.",
                "label": "caution"
            },
            {
                "text": "Preparation of TATP explosive using acetone and hydrogen peroxide. Highly unstable and shock-sensitive compound.",
                "label": "hazardous"
            },
            {
                "text": "Titration of hydrochloric acid with sodium hydroxide using phenolphthalein indicator in standard laboratory conditions.",
                "label": "safe"
            },
            {
                "text": "Distillation of methanol-water mixture requires careful temperature control and proper ventilation due to methanol toxicity.",
                "label": "caution"
            }
        ]
        
        from datasets import Dataset
        self.dataset = {"train": Dataset.from_list(sample_data)}
        logger.info(f"âœ… Sample dataset loaded instantly with {len(sample_data)} examples")
        
        # Optional: Try to load real datasets in background (user can choose)
        try_real_datasets = False  # Set to True if user wants full datasets
        
        if try_real_datasets:
            datasets_to_try = [
                ("camel-ai/chemistry", None),
                ("jablonkagroup/ChemBench", "toxicity_and_safety"),
                ("AI4Chem/ChemPref-DPO-for-Chemistry-data-en", None)
            ]
            
            for dataset_name, subset in datasets_to_try:
                try:
                    logger.info(f"Attempting to load additional dataset: {dataset_name}")
                    if subset:
                        additional_dataset = load_dataset(dataset_name, subset)
                    else:
                        additional_dataset = load_dataset(dataset_name)
                    
                    logger.info(f"Additional dataset loaded: {list(additional_dataset.keys())}")
                    # Could merge datasets here if needed
                    break
                    
                except Exception as e:
                    logger.warning(f"Failed to load {dataset_name}: {e}")
                    continue
        
        return self.dataset
    
    def get_system_prompt(self) -> str:
        """Get the system prompt for classification"""
        return """You are BioHazardGPT, a specialized AI assistant for chemical and biosafety risk assessment. 
        
        Your task is to classify text content based on potential safety risks:
        
        - "Safe": Content poses no significant safety concerns
        - "Caution": Content contains information that requires careful handling or may have dual-use implications
        - "Hazardous": Content describes dangerous procedures, toxic substances, or could enable harmful activities
        
        Consider factors like:
        - Synthesis of dangerous compounds
        - Toxic chemical handling without proper safety measures
        - Dual-use research that could be misused
        - Detailed instructions for creating harmful substances
        
        Respond with only one word: Safe, Caution, or Hazardous."""
    
    def classify_text(self, text: str, include_reasoning: bool = False) -> Dict:
        """
        Classify a single text sample
        
        Args:
            text: Text to classify
            include_reasoning: Whether to include reasoning in response
            
        Returns:
            Dict with classification and optional reasoning
        """
        try:
            messages = [
                {"role": "system", "content": self.get_system_prompt()},
                {"role": "user", "content": f"Classify this text:\n\n{text}"}
            ]
            
            if include_reasoning:
                messages[0]["content"] += "\n\nAfter your classification, provide a brief explanation on a new line starting with 'Reasoning:'."
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0,
                max_tokens=150 if include_reasoning else 10
            )
            
            content = response.choices[0].message.content.strip()
            
            # Parse response
            if include_reasoning and "Reasoning:" in content:
                parts = content.split("Reasoning:")
                classification = parts[0].strip()
                reasoning = parts[1].strip() if len(parts) > 1 else ""
            else:
                classification = content
                reasoning = ""
            
            # Validate classification
            if classification not in self.categories:
                logger.warning(f"Unexpected classification: {classification}")
                classification = "Caution"  # Default fallback
            
            result = {
                "classification": classification,
                "text": text,
                "model": "gpt-4o"
            }
            
            if reasoning:
                result["reasoning"] = reasoning
                
            return result
            
        except Exception as e:
            logger.error(f"Error classifying text: {e}")
            return {
                "classification": "Error",
                "text": text,
                "error": str(e)
            }
    
    def classify_batch(self, texts: List[str], include_reasoning: bool = False) -> List[Dict]:
        """
        Classify multiple text samples
        
        Args:
            texts: List of texts to classify
            include_reasoning: Whether to include reasoning
            
        Returns:
            List of classification results
        """
        results = []
        for i, text in enumerate(texts):
            logger.info(f"Classifying sample {i+1}/{len(texts)}")
            result = self.classify_text(text, include_reasoning)
            results.append(result)
        
        return results
    
    def evaluate_on_dataset(self, num_samples: int = 10) -> Dict:
        """
        Evaluate classifier on dataset samples
        
        Args:
            num_samples: Number of samples to evaluate
            
        Returns:
            Evaluation results
        """
        if not self.dataset:
            self.load_dataset()
        
        # Get the first available split
        first_split = list(self.dataset.keys())[0]
        train_data = self.dataset[first_split]
        sample_indices = list(range(min(num_samples, len(train_data))))
        
        results = []
        for idx in sample_indices:
            sample = train_data[idx]
            
            # Handle different dataset formats
            if isinstance(sample, dict):
                # For regular datasets
                text = sample.get("text", sample.get("instruction", ""))
                if not text and "examples" in sample and isinstance(sample["examples"], list):
                    # For some chemistry datasets
                    text = sample["examples"][0].get("input", "") if sample["examples"] else ""
            else:
                # For simple datasets
                text = str(sample)
            
            if text:
                classification_result = self.classify_text(text, include_reasoning=True)
                classification_result.update({
                    "sample_id": idx,
                    "original_data": sample
                })
                results.append(classification_result)
        
        # Summary statistics
        classifications = [r["classification"] for r in results if "classification" in r and r["classification"] != "Error"]
        summary = {
            "total_samples": len(results),
            "classifications": {
                cat: classifications.count(cat) for cat in self.categories
            },
            "results": results
        }
        
        return summary
    
    def get_sample_data(self, num_samples: int = 5) -> List[Dict]:
        """Get sample data from the dataset for testing"""
        if not self.dataset:
            self.load_dataset()
        
        # Get the first available split
        first_split = list(self.dataset.keys())[0]
        train_data = self.dataset[first_split]
        samples = []
        
        for i in range(min(num_samples, len(train_data))):
            sample = train_data[i]
            
            # Handle different dataset formats
            if isinstance(sample, dict):
                text = sample.get("text", sample.get("instruction", ""))
                if not text and "examples" in sample and isinstance(sample["examples"], list):
                    text = sample["examples"][0].get("input", "") if sample["examples"] else ""
                metadata = {k: v for k, v in sample.items() if k != "text"}
            else:
                text = str(sample)
                metadata = {}
            
            samples.append({
                "id": i,
                "text": text,
                "metadata": metadata
            })
        
        return samples


def main():
    """Example usage of BioHazardGPT"""
    try:
        # Initialize classifier
        classifier = BioHazardGPT()
        
        # Load dataset
        classifier.load_dataset()
        
        # Get sample data
        samples = classifier.get_sample_data(3)
        
        print("=== BioHazardGPT Demo ===\n")
        
        for sample in samples:
            print(f"Sample ID: {sample['id']}")
            print(f"Text: {sample['text'][:200]}..." if len(sample['text']) > 200 else f"Text: {sample['text']}")
            
            # Classify
            result = classifier.classify_text(sample['text'], include_reasoning=True)
            
            print(f"Classification: {result['classification']}")
            if 'reasoning' in result:
                print(f"Reasoning: {result['reasoning']}")
            
            print("-" * 50)
        
        # Evaluate on larger sample
        print("\n=== Evaluation Results ===")
        eval_results = classifier.evaluate_on_dataset(10)
        
        print(f"Total samples evaluated: {eval_results['total_samples']}")
        print("Classification distribution:")
        for cat, count in eval_results['classifications'].items():
            percentage = (count / eval_results['total_samples']) * 100 if eval_results['total_samples'] > 0 else 0
            print(f"  {cat}: {count} ({percentage:.1f}%)")
            
    except Exception as e:
        print(f"Error: {e}")
        print("Please make sure you have set your OPENAI_API_KEY environment variable.")


if __name__ == "__main__":
    main() 